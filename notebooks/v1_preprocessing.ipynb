{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840f7d94",
   "metadata": {},
   "source": [
    "## <span style=\"color: Orange;\">**Preprocessing**</span>\n",
    "\n",
    "In this section of our project, we will apply necessary preprocessing to the dataset to prepare it for downstream modeling. Our modeling strategy will include:\n",
    "\n",
    "- Baseline model: **Random Forest Classifier**, effective for small datasets and handling nonlinear relationships and class imbalance.  \n",
    "- Second model: **Support Vector Machine (SVM)**, motivated by observed distinct class separation during EDA (t-SNE visualization).  \n",
    "- Third model: A boosted tree model such as **XGBoost, CatBoost, or LightGBM** for potentially improved performance.  \n",
    "- Final model: A **Neural Network** for further experimentation and comparison.  \n",
    "\n",
    "**Overview of Preprocessing**\n",
    "- [Loading Configuration and Dataset](#section-i-loading-configuration-and-dataset)\n",
    "- [Adding Engineered Features](#section-ii-adding-engineered-features)\n",
    "- [Loading Feature Sets](#section-iii-loading-feature-sets)\n",
    "- [Preprocessing Pipeline Setup](#section-iv-preprocessing-pipeline-setup)\n",
    "- [Splitting Dataset into Train, Validation, and Test Sets](#section-v-splitting-dataset-into-train-validation-and-test-sets)\n",
    "- [Saving Processed Splits](#section-vi-saving-processed-splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd2146",
   "metadata": {},
   "source": [
    "### <span style=\"color: Orange;\">**Section I: Loading Configuration and Dataset**</span>\n",
    "In this section, we load the necessary configuration files to set up the paths and parameters for the notebook. We then import the Breast Cancer Wisconsin dataset, removing irrelevant columns such as IDs and unnamed index columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a00ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the configuation and the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "# Loading the configuration files for the notebook\n",
    "with open(\"../notebook_config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset_path = config['paths']['dataset_path']\n",
    "eng_features_path = config['paths']['feature_set_eng_path']\n",
    "raw_features_path = config['paths']['feature_set_raw_path']\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.drop(columns=['id', 'Unnamed: 32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c893c",
   "metadata": {},
   "source": [
    "### <span style=\"color: Orange;\">**Section II: Adding Engineered Features**</span>\n",
    "Here, we augment the raw dataset with additional domain-informed engineered features by combining existing variables. These new features aim to enrich the dataset and potentially improve model performance in downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ee9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the engineered features\n",
    "df['con_worst_per_area'] = df['concavity_worst'] / df['area_worst']\n",
    "df['con_mean_per_area'] = df['concavity_mean'] / df['area_mean']\n",
    "\n",
    "df['fd_worst_perimeter'] = df['fractal_dimension_worst']*df['perimeter_worst']\n",
    "df['fd_mean_perimeter'] = df['fractal_dimension_mean']*df['perimeter_mean']\n",
    "\n",
    "df['sym_worst_compactness'] = df['symmetry_worst']*df['compactness_worst']\n",
    "df['sym_mean_compactness'] = df['symmetry_mean']*df['compactness_mean']\n",
    "\n",
    "df['smooth_worst_radius'] = df['smoothness_worst']*df['radius_worst']\n",
    "df['smooth_worst_radius'] = df['smoothness_mean']*df['radius_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad47b5f",
   "metadata": {},
   "source": [
    "### <span style=\"color: Orange;\">**Section III: Loading Feature Sets**</span>\n",
    "We load the predefined lists of engineered and raw features, which were selected based on previous exploratory data analysis and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967c46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the two feature sets\n",
    "with open(eng_features_path, 'rb') as f:\n",
    "    eng_feature_lst = pickle.load(f)\n",
    "with open(raw_features_path, 'rb') as f:\n",
    "    raw_feature_lst = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d7d66",
   "metadata": {},
   "source": [
    "### <span style=\"color: Orange;\">**Section IV: Preprocessing Pipeline Setup**</span>\n",
    "This section describes our preprocessing strategy where we apply a robust scaler to all numeric features. Robust scaling helps reduce the impact of outliers by using statistics that are robust to extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "231784ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a preprocessing pipeline for our dataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lets apply robust scaling to all of the numeric features\n",
    "scaler = RobustScaler()\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns.to_list()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731ce93",
   "metadata": {},
   "source": [
    "### <span style=\"color: Orange;\">**Section V: Splitting Dataset into Train, Validation, and Test Sets**</span>\n",
    "We split the dataset into training, validation, and test subsets with stratification on the target to maintain class balance. Additionally, we prepare separate feature matrices for engineered and raw feature sets for model development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89470b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting as features (X) and target label (y)\n",
    "X = df.drop(columns=['diagnosis'])\n",
    "y = df['diagnosis']\n",
    "y = y.map({\"B\": 0, \"M\": 1})\n",
    "\n",
    "\n",
    "# Lets split the data based on the feature sets into two sets of temp/test splits followed by train/val\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, stratify=y, train_size=0.8, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, stratify=y_temp, train_size=0.8, random_state=42)\n",
    "\n",
    "X_train_eng = X_train[eng_feature_lst]\n",
    "X_val_eng = X_val[eng_feature_lst]\n",
    "X_test_eng = X_test[eng_feature_lst]\n",
    "y_train_eng = y_train\n",
    "y_val_eng = y_val\n",
    "y_test_eng = y_test\n",
    "\n",
    "X_train_raw = X_train[raw_feature_lst]\n",
    "X_val_raw = X_val[raw_feature_lst]\n",
    "X_test_raw = X_test[raw_feature_lst]\n",
    "y_train_raw = y_train\n",
    "y_val_raw = y_val\n",
    "y_test_raw = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496613ec",
   "metadata": {},
   "source": [
    "### <span style=\"color: Orange;\">**Section VI: Saving Processed Splits**</span>\n",
    "To facilitate reproducibility and further experimentation, all train, validation, and test splits—with their respective engineered and raw feature subsets—are saved as CSV files. This ensures a consistent and easy-to-share data pipeline for modeling stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfd8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Save the splits as csv files\n",
    "X_train_eng.to_csv('../dataset/X_train_eng.csv', index=False)\n",
    "X_val_eng.to_csv('../dataset/X_val_eng.csv', index=False)\n",
    "X_test_eng.to_csv(\"../dataset/X_test_eng.csv\", index=False)\n",
    "y_train_eng.to_csv('../dataset/y_train_eng.csv', index=False)\n",
    "y_val_eng.to_csv('../dataset/y_val_eng.csv', index=False)\n",
    "y_test_eng.to_csv(\"../dataset/y_test_eng.csv\", index=False)\n",
    "\n",
    "# Save raw features and labels\n",
    "X_train_raw.to_csv('../dataset/X_train_raw.csv', index=False)\n",
    "X_val_raw.to_csv('../dataset/X_val_raw.csv', index=False)\n",
    "X_test_raw.to_csv(\"../dataset/X_test_raw.csv\", index=False)\n",
    "y_train_raw.to_csv('../dataset/y_train_raw.csv', index=False)\n",
    "y_val_raw.to_csv('../dataset/y_val_raw.csv', index=False)\n",
    "y_test_raw.to_csv(\"../dataset/y_test_raw.csv\", index=False)\n",
    "\n",
    "# Save main splits as well\n",
    "X_train.to_csv(\"../dataset/X_train.csv\", index=False)\n",
    "X_val.to_csv(\"../dataset/X_val.csv\", index=False)\n",
    "X_test.to_csv(\"../dataset/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"../dataset/y_train.csv\", index=False)\n",
    "y_val.to_csv(\"../dataset/y_val.csv\", index=False)\n",
    "y_test.to_csv(\"../dataset/y_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
